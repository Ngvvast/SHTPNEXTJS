The Emergence of Generative AI
2.3.1 Overview
Generative Artificial Intelligence (GenAI), encompassing technologies like Large Language Models (LLMs) capable of producing text, code, and conversation, and tools that generate realistic images, audio, and video 16, presents both transformative opportunities and significant new cybersecurity risks for the healthcare sector.2 While AI holds potential for improving diagnostics, streamlining administration, and accelerating research 2, its capabilities can also be weaponized by malicious actors, and the AI systems themselves introduce novel vulnerabilities.17 National bodies like the NCSC are actively researching and providing guidance on these risks.16 UK government policy is also evolving, with the establishment of the AI Security Institute (formerly AI Safety Institute) to focus on serious AI risks with security implications 18, and the Information Commissioner's Office (ICO) issuing guidance on data protection considerations for GenAI, particularly concerning training data, accuracy, and transparency.106 The risks manifest in two primary ways: AI enhancing existing cyber threats and the emergence of new attack vectors targeting the AI models themselves.
2.3.2 Risk 1: AI-Powered Social Engineering & Deepfake Threats
●	Description: GenAI tools dramatically lower the cost and skill required to create highly convincing and personalized social engineering attacks at scale.32 This includes crafting sophisticated phishing emails that evade traditional filters, generating tailored pretexting scenarios, and producing realistic deepfake audio and video content.17 For healthcare, this means staff and patients face increasingly deceptive communications. Deepfakes pose a particular threat, potentially being used to impersonate executives, clinicians, or patients in voice calls or video conferences (including telehealth sessions), bypass biometric authentication systems, or spread medical misinformation.113
●	Vulnerabilities: The inherent difficulty for humans in distinguishing high-quality AI-generated fakes from reality 113; limitations of existing security tools (e.g., email filters) in detecting AI-generated malicious content; potential weaknesses in voice or facial biometric authentication systems against sophisticated deepfakes 114; the trust implicitly placed in voice and video communication channels 113; lack of widespread deployment of effective deepfake detection technologies.
●	Attacker Techniques: Mass generation of highly personalized spear-phishing emails using LLMs; creating deepfake voice clones of executives or finance staff to authorize fraudulent wire transfers or request sensitive data 113; using deepfake video avatars in virtual meetings or telehealth calls to impersonate individuals for fraud or information gathering 113; generating fake expert endorsements or patient testimonials to promote fraudulent treatments or spread health misinformation 115; automating social engineering interactions using AI chatbots.39
●	Impact: Significantly increased success rates for phishing and social engineering attacks leading to credential theft, malware infections, and data breaches; substantial financial losses from deepfake-enabled fraud (e.g., CEO fraud, invoice redirection) 113; unauthorized access to systems or sensitive information through successful impersonation; erosion of trust in digital communication channels, telemedicine platforms, and potentially healthcare professionals themselves 115; reputational damage to individuals or organisations targeted by defamatory deepfakes.115
●	CISSP Domain Alignment:
○	Security Operations: Incident Detection, Security Monitoring.
○	Security and Risk Management: Security Awareness and Training, Risk Management.
○	Identity and Access Management: Authentication Methods, Identity Verification.
○	Communication and Network Security: Email Security, Voice and Video Security.
●	Implications & Considerations: The advent of convincing deepfakes poses a fundamental challenge to established identity verification practices in healthcare. Traditional methods, including knowledge-based questions, static biometrics (voiceprints, facial recognition), and even some forms of multi-factor authentication (MFA) involving voice or video confirmation, become vulnerable to spoofing.114 This necessitates a shift towards more robust techniques, such as incorporating sophisticated liveness detection into biometric systems to differentiate between a real person and a synthetic representation, or potentially moving towards continuous authentication models that monitor user behaviour over time rather than relying on single point-in-time checks. Additionally, GenAI introduces a unique risk through its propensity to "hallucinate" – generating outputs that sound plausible but are factually incorrect or nonsensical.16 In a healthcare context, where AI might be used for clinical documentation, research summarization, or even as part of decision support tools 2, reliance on such inaccurate information without rigorous human validation could lead to misdiagnosis, inappropriate treatment decisions, or the propagation of medical misinformation.115 This risk blurs the line between a cybersecurity concern (data integrity, misinformation) and a direct clinical safety issue, demanding careful governance and mandatory human oversight in critical applications.17
2.3.3 Risk 2: Data Poisoning & Integrity Risks for Healthcare AI Models
●	Description: The effectiveness and safety of AI models used in healthcare (e.g., for diagnostic image analysis, predicting patient risk, treatment recommendation, drug discovery) depend critically on the integrity of their training data.119 Data poisoning attacks involve malicious actors deliberately corrupting this training data to manipulate the model's behaviour.17 This could involve introducing biases to cause discriminatory outcomes, creating hidden backdoors that cause the model to misbehave under specific conditions (e.g., when encountering a specific trigger word or image pattern), or simply degrading the model's overall accuracy and reliability.120 Such attacks undermine the trustworthiness and safety of AI in clinical settings. Furthermore, adversarial attacks, which involve making subtle, often imperceptible changes to the input data at the time of inference, can also trick AI models into making incorrect classifications or predictions.39
●	Vulnerabilities: Heavy reliance of deep learning models on vast amounts of training data, often sourced from diverse and potentially unverified origins (including web scraping) 16; the inherent complexity and 'black box' nature of some AI models, making it difficult to detect subtle data manipulations or understand their impact 39; inadequate data validation, sanitation, and provenance tracking in AI development pipelines 99; vulnerabilities specific to distributed or federated learning approaches where multiple parties contribute data 119; the difficulty of detecting 'clean-label' poisoning attacks where the malicious data itself doesn't appear anomalous.119
●	Attacker Techniques: Injecting data with incorrect labels (label flipping) 119; adding carefully crafted malicious data points to the training set (data injection, backdoor poisoning) 120; subtly altering existing training data features without changing labels (input modification/clean-label attacks) 119; manipulating the semantic relationships within data (semantic poisoning) 120; modifying input data during inference to cause misclassification (adversarial examples).39 A specific healthcare example involves poisoning a clinical LLM's training data to make it recommend an incorrect or harmful medication when triggered by certain patient query patterns.124
●	Impact: Incorrect AI-assisted medical diagnoses leading to delayed or improper treatment; flawed treatment recommendations generated by compromised clinical decision support systems; biased allocation of healthcare resources or biased risk predictions due to poisoned models; complete erosion of clinical trust in AI tools, hindering adoption of beneficial technologies; potential for widespread patient harm if widely used foundational models or datasets are successfully poisoned 120; failure of AI-driven security tools due to poisoning.
●	CISSP Domain Alignment:
○	Software Development Security: Secure AI/ML Development Lifecycle, Data Validation, Model Testing.
○	Asset Security: Data Integrity, Information Classification, Data Governance.
○	Security and Risk Management: Risk Assessment, Data Quality Management.
○	Security Assessment and Testing: Model Validation and Verification, Security Audits of AI Systems.
●	Implications & Considerations: Data poisoning attacks represent a fundamental challenge to the integrity and trustworthiness of AI systems, particularly in high-stakes domains like healthcare.96 While traditional cybersecurity often prioritises confidentiality (preventing data theft) and availability (ensuring system uptime), data poisoning targets the core logic and reliability of the AI model itself. A poisoned AI might remain available and its data confidential, yet produce dangerously incorrect or biased outputs. Detecting and mitigating these attacks requires specialized techniques beyond standard network or endpoint security, such as robust data validation, anomaly detection in training data, adversarial training, and model monitoring.99 Furthermore, the increasing use of third-party datasets, pre-trained models, and AI-as-a-Service platforms in healthcare introduces significant supply chain risks related to data poisoning.33 Healthcare organisations deploying these external AI components may have little visibility or control over the original training data's provenance and integrity.112 A vulnerability or poisoning attack introduced during the vendor's development process becomes an inherited risk for the healthcare organisation, making thorough vendor due diligence and ongoing model validation critical, yet challenging.
